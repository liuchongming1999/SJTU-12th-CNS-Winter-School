{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "----\n",
    "### *The 12th Computational Neuroscience Winter School*\n",
    "\n",
    "# Tutorial I: Brain Inspired Computation - Based Part\n",
    "----\n",
    "__Date:__ Jan. 10, 2023\n",
    "\n",
    "__Content Creator:__ Chongming Liu"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tutorial Objectives\n",
    "\n",
    "In this notebook, we'll learn the history of the development of artificial intelligence in the last century. We gonna to learn some classical models and algorithms and with some interesting examples as exercises.\n",
    "\n",
    "1. MP Model\n",
    "1. Hebbian Learning\n",
    "1. Hopfield Model\n",
    "1. Building Up A Simple Artificial Neural Network"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "# Setup\n",
    "\n",
    "\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Import most modules and functions needed\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from scipy.stats import chi2\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['lines.linewidth'] = 2.5\n",
    "mpl.rcParams['legend.fontsize'] = 12\n",
    "mpl.rcParams['axes.labelsize'] = 12\n",
    "mpl.rcParams['axes.titlesize'] = 14\n",
    "mpl.rcParams['font.weight'] = 'bold'\n",
    "mpl.rcParams['font.size'] = 12\n",
    "mpl.rcParams['axes.labelweight'] = 'bold'\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.datasets\n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.model_selection import train_test_split \n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "# Section 1: MP Model\n",
    "\n",
    "*Reference:* \n",
    "\n",
    "*McCulloch, W.S., & Pitts, W.H. (1943). A logical calculus of the ideas immanent in nervous activity. Bulletin of Mathematical Biology, 52, 99-115.*\n",
    "\n",
    "---\n",
    "\n",
    "Warren S. Mcculloch and Walter Pitts (1943) found that single neuron can do logical operations. Based on this fact, they proposed a kind of computational model that performs like a neuron."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"./imgs/MP_model.png\" width=\"50%\">"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "MP Model can be used to do binary classification (Actually, MP model can be seen as linear classifier), since the output of the model is boolean. Here we take Wiscosin dataset as an example. Wiscosin dataset is a dataset for breast cancer, which contains 30 features about the tumor, and you need to classify that which is malignant and which is benign. \n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Wiscosin dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "breast_cancer = sklearn.datasets.load_breast_cancer()\n",
    "\n",
    "X = breast_cancer.data\n",
    "Y = breast_cancer.target\n",
    "data = pd.DataFrame(breast_cancer.data,columns=breast_cancer.feature_names)\n",
    "data['class'] = breast_cancer.target\n",
    "\n",
    "data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Spliting the data to training set and test set"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X = data.drop('class',axis=1)\n",
    "Y = data['class']\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.1, stratify=Y, random_state=0)\n",
    "\n",
    "\n",
    "X_binarise_train = X_train.apply(pd.cut, bins=2, labels=[1,0])\n",
    "X_binarise_test = X_test.apply(pd.cut, bins=2, labels=[1,0])\n",
    "\n",
    "\n",
    "X_binarise_train = X_binarise_train.values\n",
    "X_binarise_test = X_binarise_test.values\n",
    "\n",
    "\n",
    "print('Target: ',Y.mean(), Y_train.mean(), Y_test.mean())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Constrcut MP neuron\n",
    " \n",
    "We should note that there didn't exist an training algorithm for us to find out $w$, so here we would fix all the weight to $1$, and try to search a better $\\theta$ for classification. The input-output relation becomes:\n",
    "$$\\hat y_i=\\Theta(\\sum_{j}^n x_i^j-\\theta)$$\n",
    "Since there are totally $n$ features, here we investigate the best integer threshold between $[0,n]$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class MPNeuron:\n",
    "    def __init__(self):\n",
    "        self.b = None\n",
    "    \n",
    "    def model(self,x):\n",
    "        return (sum(x) >= self.b)\n",
    "    \n",
    "    def predict(self,X):\n",
    "        Y = []\n",
    "        for x in X:\n",
    "            Y.append(self.model(x))\n",
    "        return np.array(Y)\n",
    "    \n",
    "    def fit(self,X,Y):\n",
    "        accuracy = {}\n",
    "        for b in range(X.shape[1] + 1):\n",
    "            self.b = b\n",
    "            Y_pred = self.predict(X)\n",
    "            accuracy[b] = accuracy_score(Y_pred,Y)\n",
    "            \n",
    "        best_b = max(accuracy, key = accuracy.get)\n",
    "        self.b = best_b\n",
    "        \n",
    "        print('Best threshold:', best_b)\n",
    "        print('Best accuracy: {:.2f}'.format(accuracy[best_b]))\n",
    "\n",
    "        \n",
    "mp_neuron = MPNeuron()\n",
    "mp_neuron.fit(X_binarise_train,Y_train)\n",
    "\n",
    "w = mp_neuron.predict(X_binarise_test)\n",
    "print('Test accuracy: {:.2f}'.format(accuracy_score(w,Y_test)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "# Section 2: Hebbian Learning\n",
    "\n",
    "*Reference:* \n",
    "\n",
    "*Hebb, D.O. (1949). The Organization of Behavior: A Neuropsychological Theory.*\n",
    "\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Hebb's rule: \"Fire together, wire together.\" Consider using Hebb's rule in the MP model:\n",
    "\n",
    "$$\n",
    "y(x)=\\Theta(w^Tx-\\theta)\n",
    "$$\n",
    "\n",
    "then, a sequence of inputs $(x_i)_{i=1}^n$ are given to the model, the weights would updates as\n",
    "\n",
    "$$\n",
    "w_{t+1}=w_{t}+\\eta y(x_i)x_i\n",
    "$$\n",
    "\n",
    "So Hebbian learning is an algorithm for unsupervised learning. If we consider a simplified MP model:\n",
    "\n",
    "$$y(x)=w^Tx$$\n",
    "\n",
    "And we update the weight only when the model has seen all data, then the updating rule becomes:\n",
    "\n",
    "$$w_{t+1}=w_{t}+\\eta \\sum_{i=1}^ny(x_i)x_i=(I+\\eta \\sum_{i=1}^n x_ix_i^T)w_{t}$$\n",
    "\n",
    "So we have\n",
    "\n",
    "$$w_{t}=(I+\\eta C)^t w_{0}$$\n",
    "\n",
    "where $C=\\sum_{i=1}^n x_ix_i^T$ is the covariance matrix of data. Then we know the weight vector would finally converge to the direction of leading eigenvector of $C$, as long as  the initial iterate has a projection onto the leading eigenvector."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# generate data matrix\n",
    "X = np.random.randn(10000,100)\n",
    "mean_x = np.ones(100)\n",
    "X_data = mean_x + X\n",
    "\n",
    "# Compute covariance matrix\n",
    "C = np.matmul(X_data.T, X_data)\n",
    "\n",
    "w = np.random.randn(100)\n",
    "# Hebbian learning\n",
    "eta = 0.1\n",
    "for step in range(10000):\n",
    "    w += eta*np.matmul(C,w)\n",
    "    w = w/np.linalg.norm(w)\n",
    "    \n",
    "# Compute the absolute cos between weight the mean of data\n",
    "abs_cos = abs(np.matmul(w,mean_x))/(np.linalg.norm(mean_x))\n",
    "print('absolute cos between weight the mean of data: {:.2f}'.format(abs_cos))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "# Section 3: Hopfield Model\n",
    "\n",
    "*Reference:* \n",
    "\n",
    "*Hopfield, J.J. (1982). Neural networks and physical systems with emergent collective computational abilities. Proceedings of the National Academy of Sciences of the United States of America, 79 8, 2554-8 .*\n",
    "\n",
    "*Hopfield, J.J. (1984). Neurons with graded response have collective computational properties like those of two-state neurons. Proceedings of the National Academy of Sciences of the United States of America, 81 10, 3088-92 .*\n",
    "\n",
    "*Gerstner, W., Kistler, W.M., Naud, R., & Paninski, L. (2014). Neuronal Dynamics: From Single Neurons To Networks And Models Of Cognition.*\n",
    "    \n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The Hopfield model consists of a network of $N$ neurons, labeled by a lower index $i$, with $1\\leq i\\leq N$. Similar to MP model, neurons in the Hopfield model have only two states. A neuron $i$ is \"ON\" if its state variable takes the value $S_i=+1$ and \"OFF\" if $S_i=-1$. Neurons interact with each other with weights $w_{ij}$. The input potential of neuron $i$, influenced by the activity of other neurons is\n",
    "\n",
    "$$h_i(t)=\\sum_j w_{ij}S_{j}(t)$$\n",
    "\n",
    "The input potential at time $t$ influences the probabilistic update of the state variable $S_i$ in the next time step:\n",
    "\n",
    "$$P\\{S_i(t+\\Delta t)=+1|h_i(t)\\}=g(h_i(t))=g(\\sum_j w_{ij}S_{j}(t))$$\n",
    "\n",
    "where g is a monotonically increasing gain function with values between zero and 1. If $g$ is Heaviside function, then the dynamics are therefore deterministic and summarized by the update rule\n",
    "\n",
    "$$S_i(t+\\Delta t)=sgn[h_i(t)]$$\n",
    "\n",
    "The most powerful property of Hopfield model is that with a suitable choice of the coupling matrix $w_{ij}$,\n",
    "memory items can be retrieved by the collective dynamics. If we need the network to store and recall $M$ different patterns, patterns are labeled by the index $\\mu$ with $1\\leq \\mu\\leq M$. Each pattern $\\mu$ is define as desired configuration $\\{p_i^\\mu=\\pm 1; 1\\leq i\\leq N\\}$. The network of $N$ neurons is said to correctly represent pattern $\\mu$, if the state of all neurons $1\\leq u\\leq N$ is $S_i(t)=S_i(t+\\Delta t)=p_i^\\mu$. In other words, patterns must\n",
    "be fixed points of the dynamics."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can choose the weight as:\n",
    "\n",
    "$$w_{ij}=\\frac{1}{N}\\sum_{\\mu=1}^{M}p_i^\\mu p_j^\\mu$$\n",
    "\n",
    "Then the network stores all patterns. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Math detour\n",
    "\n",
    "Define the similarity between the current state and a pattern $\\mu$:\n",
    "\n",
    "$$m^\\mu(t) = \\frac{1}{N}\\sum_{i}p_i^\\mu S_i(t)$$\n",
    "\n",
    "Define energy function\n",
    "\n",
    "$$E(t)=-\\sum_i\\sum_j w_{ij}S_i(t)S_j(t)$$\n",
    "\n",
    "Then the change in energy by updating the state of neuron $k$ is\n",
    "\n",
    "$$E(t+\\Delta t)-E(t)=-4h_k sgn(h_k)<0$$\n",
    "\n",
    "And if we insert the definitionof the connection weights $w_{ij}=\\frac{1}{N}\\sum_{\\mu=1}^{M}p_i^\\mu p_j^\\mu$, then we find:\n",
    "\n",
    "$$E(t)=-N\\sum_{\\mu}(m^\\mu(t))^2$$\n",
    "\n",
    "The maximum value of the overlap with a fixed pattern $\\nu$ is $m^\\nu = 1$. Moreover, for\n",
    "random patterns, the correlations between patterns are small. Therefore, if $m^{\\nu} = 1$ (i.e.,\n",
    "recall of pattern $\\nu$) the overlap with other patterns $\\mu\\neq \\nu$ is $m^\\mu\\approx 0$. Therefore, the energy landscape can be visualized with multiple minima of the same depth, each minimum corresponding to retrieval of one of the patterns"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### An example"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# generate 'E' pattern\n",
    "pattern_E = np.array([1,1,1,1,-1,1,-1,-1,-1,-1,1,1,1,-1,-1,1,-1,-1,-1,-1,1,1,1,1,-1]).reshape(25,1)\n",
    "\n",
    "plt.figure(figsize=(1.5,1.5))\n",
    "plt.imshow(pattern_E.reshape(5,5), cmap = plt.cm.binary)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# generate connection weight\n",
    "weight_matrix = np.matmul(pattern_E, pattern_E.T)/25\n",
    "\n",
    "np.random.seed(2)\n",
    "# initial state\n",
    "state = np.sign(np.random.randn(25))\n",
    "fig,ax=plt.subplots(1,2, layout=\"constrained\", figsize=(3,6))\n",
    "ax[0].imshow(state.reshape(5,5), cmap = plt.cm.binary)\n",
    "\n",
    "# One step for hopfield iteration\n",
    "state = np.sign(np.matmul(weight_matrix, state))\n",
    "ax[1].imshow(state.reshape(5,5), cmap = plt.cm.binary)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# generate '7' pattern\n",
    "pattern_7 = np.array([-1,-1,1,1,1,-1,-1,-1,-1,1,-1,-1,-1,-1,1,-1,-1,-1,-1,1,-1,-1,-1,-1,1]).reshape(25,1)\n",
    "\n",
    "plt.figure(figsize=(1.5,1.5))\n",
    "plt.imshow(pattern_7.reshape(5,5), cmap = plt.cm.binary)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# generate connection weight\n",
    "weight_matrix = (np.matmul(pattern_E, pattern_E.T)+np.matmul(pattern_7, pattern_7.T))/25\n",
    "\n",
    "# initial state\n",
    "state = np.array([1,1,-1,-1,-1,1,-1,-1,-1,-1,1,1,1,-1,-1,1,-1,-1,-1,-1,1,1,1,1,-1])\n",
    "fig,ax=plt.subplots(1,2, layout=\"constrained\", figsize=(3,6))\n",
    "ax[0].imshow(state.reshape(5,5), cmap = plt.cm.binary)\n",
    "ax[0].set_title('initial state')\n",
    "\n",
    "# One step for hopfield iteration\n",
    "state = np.sign(np.matmul(weight_matrix, state))\n",
    "ax[1].imshow(state.reshape(5,5), cmap = plt.cm.binary)\n",
    "ax[1].set_title('final state')\n",
    "\n",
    "# another initial state\n",
    "state = np.array([1,1,1,1,1,-1,-1,-1,-1,1,-1,-1,-1,-1,1,-1,-1,-1,-1,1,-1,-1,-1,-1,1])\n",
    "fig,ax=plt.subplots(1,2, layout=\"constrained\", figsize=(3,6))\n",
    "ax[0].imshow(state.reshape(5,5), cmap = plt.cm.binary)\n",
    "\n",
    "# One step for hopfield iteration\n",
    "state = np.sign(np.matmul(weight_matrix, state))\n",
    "ax[1].imshow(state.reshape(5,5), cmap = plt.cm.binary)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "# Section 4: Building Up A Simple Artificial Neural Network\n",
    "\n",
    "*Reference:* \n",
    "\n",
    "*Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., KÃ¶pf, A., Yang, E., DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., & Chintala, S. (2019). PyTorch: An Imperative Style, High-Performance Deep Learning Library. Neural Information Processing Systems.*\n",
    "\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Modern artificial neural network:\n",
    "<img src=\"./imgs/simple_network.png\" width=\"50%\">\n",
    "\n",
    "$$\n",
    "f(x) = w_L\\sigma(w_{L-1}\\sigma(\\cdots(w_2*\\sigma(w_1*x+b_1)+b_2))+b_{L-1})+b_L\n",
    "$$\n",
    "\n",
    "Mostly used in supervised learning, with loss function and back propagation. ([A useful tool for plotting the schematic of network](http://alexlenail.me/NN-SVG/index.html))\n",
    "\n",
    "When doing classification tasks, we usually use crossentropy loss:\n",
    "\n",
    "$$\n",
    "\\mathcal L(x_n,\\theta) = -\\log(\\frac{\\exp(o_{x_n,y_n})}{\\sum_{c=1}^{C}\\exp(o_{x_n,c})})\n",
    "$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Construct the network\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(30, 50),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.Linear(50, 2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):                             \n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "breast_cancer = sklearn.datasets.load_breast_cancer()  \n",
    "X = breast_cancer.data\n",
    "Y = breast_cancer.target\n",
    "\n",
    "# Prepocessing of the data\n",
    "for i in range(X.shape[1]):\n",
    "    X[:,i] = X[:,i]/max(X[:,i])\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.1, stratify=Y, random_state=0)\n",
    "\n",
    "train_dataset = torch.from_numpy(X_train)\n",
    "valid_dataset = torch.from_numpy(X_test)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=X_train.shape[0], shuffle=False)\n",
    "valid_loader = torch.utils.data.DataLoader(dataset=valid_dataset, batch_size=X_test.shape[0])\n",
    "Y_train = torch.from_numpy(Y_train)\n",
    "Y_test = torch.from_numpy(Y_test)\n",
    "\n",
    "model = SimpleNet()\n",
    "print(model)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Training the network\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "train_accuracy = []\n",
    "test_accuracy = []\n",
    "best_prec = 0\n",
    "min_loss = 1\n",
    "\n",
    "for epoch in range(0, 20000):\n",
    "\n",
    "    model.train()\n",
    "    # train for one epoch\n",
    "    for i, input in enumerate(train_loader):\n",
    "                    \n",
    "        # compute output\n",
    "        output = model(input.to(torch.float32))\n",
    "        loss = criterion(output, Y_train)\n",
    "\n",
    "    # print training accuracy and test accuracy after every 100 epochs.\n",
    "        if epoch % 200 == 0:\n",
    "\n",
    "            train_total = 0\n",
    "            train_correct = 0\n",
    "            train_loss = 0\n",
    "\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            train_total =output.shape[0]\n",
    "            train_correct = (predicted == Y_train).sum().item()\n",
    "            prec = train_correct / train_total\n",
    "\n",
    "            train_accuracy.append(prec)\n",
    "            print('Epoch [{}/{}], Loss: {:.5f}, Train_Acc:{:.2f}%'.format(epoch, 20000, loss, prec*100))\n",
    "\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward() \n",
    "        optimizer.step()\n",
    "        \n",
    "    if (epoch+1) % 200 == 0:\n",
    "        \n",
    "        model.eval()\n",
    "        valid_correct = 0\n",
    "        valid_total = 0\n",
    "        with torch.no_grad():\n",
    "            total_loss = 0\n",
    "            for j, input in enumerate(valid_loader):\n",
    "                output = model(input.to(torch.float32))\n",
    "\n",
    "                _, predicted = torch.max(output.data, 1)\n",
    "                valid_total = output.shape[0]\n",
    "                valid_correct = (predicted == Y_test).sum().item()\n",
    "\n",
    "                prec = valid_correct / valid_total\n",
    "                print('Accuary on test images:{:.2f}%, loss:{:.2f}'.format(prec*100, loss))\n",
    "                test_accuracy.append(prec)\n",
    "                best_prec = max(prec, best_prec)\n",
    "\n",
    "print('Best accuracy is: {:.2f}%'.format(best_prec*100))\n",
    "\n",
    "plt.figure(figsize=(3.5,2))\n",
    "plt.plot(200*torch.arange(len(test_accuracy)), test_accuracy,linewidth=2)\n",
    "plt.xlabel('# epochs')\n",
    "plt.ylabel('Test accuracy')\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "# Summary\n",
    "\n",
    "- In this tutorial, we learned several famous brain inspired models and algorithms in terms of their basic idea and how to use them in a specific task.\n",
    "- MP model can be seen as the basic units in the modern artificial neural network. Although it is simple and only possess binary input and output, it can do some simple tasks such as binary classification.\n",
    "- Hebb's rule are based on the biological fact of synaptic plasticity rule, it is an algorithm for unsupervised learning, which can recognize the structure in the data.\n",
    "- Hopfield model is an abstract model of memory retrieval. After a cue with a partial overlap with one of the stored memory patterns is presented, the memory item is retrieved. Because the Hopfield model has symmetric synaptic connections, memory retrieval can be visualized as downhill movement in an energy landscape.\n",
    "- We also learned how to use pytorch to construct an artificial neural network, and use this network to do classification tasks.\n",
    "\n",
    "---"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (torch-env)",
   "language": "python",
   "name": "torch-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}